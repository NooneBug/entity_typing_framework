# this yaml file contains all possible parameters with description for the modules in this library
model:
  class_path: entity_typing_framework.main_module.main_module.MainModule
  # path to class of the main module, which contains the dataset_manager and the Entity_typing_network
  init_args:
    dataset_manager:
      class_path: entity_typing_framework.dataset_classes.dataset_managers.DatasetManager
      # path to class of the dataset manager class, which acquire, format, tokenize datasets and build the dataloader (according to the Entity_typing_network)
      init_args:
        dataset_paths:
          # path to the dataset reader class, which acquire the data
          class_path: entity_typing_framework.dataset_classes.datasets.BaseDataset
          init_args:
            # datasets partitions has to be named (train, dev, test) and their path has to be passed as parameter. custom names are allowed but have to be managed by the dataset manager.
            train: datasets/toy_datasets/toy_bbn_train.json
            dev: datasets/toy_datasets/toy_bbn_dev.json
            test: datasets/toy_datasets/toy_bbn_test.json
        tokenizer_params:
          class_path: entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset
          #path to the tokenizer class, a valid huggingface model will tokenize each input sentence following the truncation parameters
          init_args:    
            bertlike_model_name: distilbert-base-uncased
            # a valid huggingface model name, will be used to tokenize each input sentence. Sentences can be padded
            max_mention_words: 1
            # truncation parameter: the mention is truncated to obtain a mention with the first max_mention_words words   
            max_right_words: 2
            # truncation parameter: the right context is truncated to obtain a right context with the first max_right_words words   
            max_left_words: 2
            # truncation parameter: the left context is truncated to obtain a left context with the last max_left_words words   
        dataloader_params:
          # for each dataset partition, define the dataloader class and the params. Note: the partition name are to be equal to the one in dataset_paths 
          train:
          #partition name
            class_path: torch.utils.data.dataloader.DataLoader
            # dataloader class
            init_args:
              batch_size: 512
              # batch size for dataloader
              shuffle: True
              # shuffle each epoch
          dev:
            class_path: torch.utils.data.dataloader.DataLoader
            init_args:
              batch_size: 512
          test:
            class_path: torch.utils.data.dataloader.DataLoader
            init_args:
              batch_size: 512
    ET_Network_params:
      class_path: entity_typing_framework.EntityTypingNetwork_classes.base_network.BaseEntityTypingNetwork
      # path to the Entity Typing Network Class, the lighting module that has to encode the input and produce a representation that can be interpreted as set of types (e.g. one hot representation, general vector if types are represented through vectors ... )
      init_args:
        network_params:
        # dictionary of params, each pair in the dictionary contains the params of a module of the ET_Network
          encoder_params:
          # params of the encoder, the module that has to represent the tokenized input
            class_path: entity_typing_framework.EntityTypingNetwork_classes.input_encoders.DistilBERTEncoder
            # path to the Encoder's class
            init_args:
              bertlike_model_name: distilbert-base-uncased
              # name of the huggingface pretrained model that has to be used (DistilBERTEncoder has also a default value for this param, so this param is redundant)
              freeze_encoder: True
              # freeze or not encoder weights
          type_encoder_params:
          # params of the type encode, the module that has to represent types
            class_path: entity_typing_framework.EntityTypingNetwork_classes.type_encoders.OneHotTypeEncoder
            # path to the type encoder's class
            init_args:
              trainable: False
              # specifies if the type representation are to be learned during the training or not
          input_projector_params:
          # params of the input projector module that has to project the encoded input (generated by the encoder) into a joint space that contains also type representations
            class_path: entity_typing_framework.EntityTypingNetwork_classes.input_projectors.Classifier
            # path to the input projector class
            init_args:
            # the classifier has some fully connected layers, each parametrized. The first layer does not require in_feature, since the module can set its value by default. The last layer does not require the out_feature, since the module can set its value by default 
              '0':
                in_features : encoder_dim
                # number of input features of the fully connected layer. In this case, since the correct number is given by the encoder and is not presente in the yaml, this string ensure that the module will set it automatically. The module has a default value for this param, so this assignment is redundant
                out_features: 512
                # number of output features of the fully connected layer
                use_dropout: True
                # indicates if use or not the dropout
              '1':
                in_features: 512
                # number of input features of the fully connected layer
                out_features : type_number
                # number of output features of the fully connected layer. In this case, since the correct number is given by the type number and is not present in the yaml, this string ensure that the module will set it automatically. The module has a default value for this param, so this assignment is redundant
                use_dropout: False
                # indicates if use or not the dropout
                activation: sigmoid
                # indicates the name of the activation function to use
    loss:
    # parameters of the class that implements the loss
      class_path: entity_typing_framework.main_module.losses.BCELossForET
      # path to the class for the loss
    logger_params:
    # parameters of the class that implements the logger
      class_path : entity_typing_framework.main_module.custom_logger.CustomLogger
      # path to the class for the logger
      init_args : 
        project : entity_typing_framework
        # wandb project name
        entity : noonebug
        # wandb entity name