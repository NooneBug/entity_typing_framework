<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Dataset Tokenizers &mdash; entity_typing_framework 0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Datasets" href="datasets.html" />
    <link rel="prev" title="Dataset Readers" href="dataset_readers.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> entity_typing_framework
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="configuration_file.html">Configuration File</a></li>
<li class="toctree-l1"><a class="reference internal" href="package_structure.html">Package Structure</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="modules_submodules.html">Modules and submodules</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="modules_submodules.html#datasetmanager">DatasetManager</a></li>
<li class="toctree-l2"><a class="reference internal" href="modules_submodules.html#entity-typing-network">Entity Typing Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="modules_submodules.html#implemented-modules">Implemented Modules</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="modules_submodules.html#implemented-submodules">Implemented Submodules</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="dataset_readers.html">Dataset Readers</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Dataset Tokenizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="datasets.html">Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="encoders.html">Encoders</a></li>
<li class="toctree-l3"><a class="reference internal" href="type_encoders.html">Type Encoders</a></li>
<li class="toctree-l3"><a class="reference internal" href="input_projectors.html">Input Projectors</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module_dictionary.html">Module Dictionary</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">entity_typing_framework</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="modules_submodules.html">Modules and submodules</a> &raquo;</li>
      <li>Dataset Tokenizers</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/dataset_tokenizers.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="dataset-tokenizers">
<h1>Dataset Tokenizers<a class="headerlink" href="#dataset-tokenizers" title="Permalink to this headline"></a></h1>
<p>List of implemented dataset_tokenizer classes, submodule of <a class="reference internal" href="modules_submodules.html#datasetmanager"><span class="std std-ref">DatasetManager</span></a></p>
<span class="target" id="baseberttokenizeddataset"></span><dl class="py class">
<dt class="sig sig-object py" id="entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">entity_typing_framework.dataset_classes.tokenized_datasets.</span></span><span class="sig-name descname"><span class="pre">BaseBERTTokenizedDataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="dataset_readers.html#entity_typing_framework.dataset_classes.datasets.BaseDataset" title="entity_typing_framework.dataset_classes.datasets.BaseDataset"><span class="pre">entity_typing_framework.dataset_classes.datasets.BaseDataset</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">type2id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bertlike_model_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_mention_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_left_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_right_words</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">80</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset" title="Permalink to this definition"></a></dt>
<dd><p>Tokenizes a dataset using <a class="reference external" href="https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/auto#transformers.AutoTokenizer">huggingface AutoTokenizer</a></p>
<dl>
<dt>Parameters:</dt><dd><dl>
<dt>dataset:</dt><dd><p>the dataset loaded by a <a class="reference internal" href="dataset_readers.html"><span class="doc">Dataset Reader</span></a> to be tokenized</p>
<p>Note : the format of the loaded dataset by the <a class="reference internal" href="dataset_readers.html"><span class="doc">Dataset Reader</span></a> has to be in accord with the one expected by <code class="code docutils literal notranslate"><span class="pre">create_sentences_from_dataset()</span></code></p>
</dd>
<dt>type2id:</dt><dd><p>the <code class="code docutils literal notranslate"><span class="pre">type2id</span></code> dictionary created by the <a class="reference internal" href="dataset_managers.html"><span class="doc">Dataset Manager</span></a></p>
</dd>
<dt>name:</dt><dd><p>the name of the submodule, has to be specified in the <code class="code docutils literal notranslate"><span class="pre">yaml</span></code> configuration file with key <code class="code docutils literal notranslate"><span class="pre">data.tokenizer_params.name</span></code></p>
<p>this param is used by the <a class="reference internal" href="dataset_managers.html"><span class="doc">Dataset Manager</span></a> to instance the correct submodule</p>
</dd>
<dt>bertlike_model_name:</dt><dd><p>required param, used to instance a <a class="reference external" href="https://huggingface.co/docs/transformers/v4.15.0/en/model_doc/auto#transformers.AutoTokenizer">huggingface AutoTokenizer</a>.</p>
<p>this param drives the tokenizer used by the model.</p>
<p>this param has to be specified in the <code class="code docutils literal notranslate"><span class="pre">yaml</span></code> configuration file with key <code class="code docutils literal notranslate"><span class="pre">data.tokenizer_params.bertlike_model_name</span></code>. The value of this param has to be equal to the param <code class="code docutils literal notranslate"><span class="pre">model.network_params.encoder_params.bertlike_model_name</span></code>.</p>
</dd>
<dt>(optional) max_mention_words:</dt><dd><p>this param ensures that only the first <code class="code docutils literal notranslate"><span class="pre">max_mention_words</span></code> of each <cite>entity mention</cite> in the sentence are tokenized, the other words are discarded.</p>
<p>accepted values are integers and the string <code class="code docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code> to avoid the word discard</p>
</dd>
<dt>(optional) max_left_words:</dt><dd><p>this param ensures that only the last <code class="code docutils literal notranslate"><span class="pre">max_left_words</span></code> in each <cite>left context</cite> in a sentence are tokenized, the other words are discarded.</p>
<p>accepted values are integers and the string <code class="code docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code> to avoid the word discard</p>
</dd>
<dt>(optional) max_right_words:</dt><dd><p>this param ensures that only the first <code class="code docutils literal notranslate"><span class="pre">max_right_words</span></code> in each <cite>right context</cite> in a sentence are tokenized, the other words are discarded.</p>
<p>accepted values are integers and the string <code class="code docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code> to avoid the word discard</p>
</dd>
<dt>(optional) max_tokens:</dt><dd><p>this param ensures that only the first <code class="code docutils literal notranslate"><span class="pre">max_tokens</span></code> tokens in each tokenized sentence are kept, the other tokens are discarded.</p>
<p>accepted values are integers and the string <code class="code docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code> to avoid the token discard</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.compute_max_length">
<span class="sig-name descname"><span class="pre">compute_max_length</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sent</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">80</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.compute_max_length" title="Permalink to this definition"></a></dt>
<dd><p>compute the maximum number of tokens in the dataset. This method is used to set the <code class="code docutils literal notranslate"><span class="pre">max_length</span></code> parameters when calling the tokenizer.</p>
<dl class="simple">
<dt>params:</dt><dd><dl class="simple">
<dt>sent:</dt><dd><p>see <code class="code docutils literal notranslate"><span class="pre">tokenize.sentences</span></code></p>
</dd>
<dt>(optional) max_tokens:</dt><dd><p>see <code class="code docutils literal notranslate"><span class="pre">tokenize.max_tokens</span></code></p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.create_sentence">
<span class="sig-name descname"><span class="pre">create_sentence</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sent_dict</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_mention_words</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_left_words</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_right_words</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.create_sentence" title="Permalink to this definition"></a></dt>
<dd><p>composes a sentence in the dataset in the format <code class="code docutils literal notranslate"><span class="pre">&quot;mention</span> <span class="pre">[SEP]</span> <span class="pre">left_context_tokens</span> <span class="pre">[SEP]</span> <span class="pre">right_context_tokens&quot;</span></code>. This method uses <code class="code docutils literal notranslate"><span class="pre">max_mention_words,</span> <span class="pre">max_left_words</span></code> and <code class="code docutils literal notranslate"><span class="pre">max_right_words</span></code> to discard words (using <code class="code docutils literal notranslate"><span class="pre">split_and_cut_mention</span></code> and <code class="code docutils literal notranslate"><span class="pre">cut_context</span></code>).</p>
<dl class="simple">
<dt>parameters:</dt><dd><dl class="simple">
<dt>sent_dict:</dt><dd><p>a single dictionary extracted by <code class="code docutils literal notranslate"><span class="pre">extract_sentences_from_dataset</span></code></p>
</dd>
<dt>max_mention_words:</dt><dd><p>see class parameter <code class="code docutils literal notranslate"><span class="pre">max_mention_words</span></code></p>
</dd>
<dt>max_left_words:</dt><dd><p>see class parameter <code class="code docutils literal notranslate"><span class="pre">max_left_words</span></code></p>
</dd>
<dt>max_right_words:</dt><dd><p>see class parameter <code class="code docutils literal notranslate"><span class="pre">max_right_words</span></code></p>
</dd>
</dl>
</dd>
<dt>return :</dt><dd><p>a sentence in the format <code class="code docutils literal notranslate"><span class="pre">&quot;mention</span> <span class="pre">[SEP]</span> <span class="pre">left_context_tokens</span> <span class="pre">[SEP]</span> <span class="pre">right_context_tokens&quot;</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.cut_context">
<span class="sig-name descname"><span class="pre">cut_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">context</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">limit</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.cut_context" title="Permalink to this definition"></a></dt>
<dd><p>cut the left or the right context (depending on the value of <code class="code docutils literal notranslate"><span class="pre">first</span></code>)</p>
<dl>
<dt>parameters:</dt><dd><dl>
<dt>context:</dt><dd><p>the string to be cutted</p>
</dd>
<dt>limit:</dt><dd><p>how many words maintain in <code class="code docutils literal notranslate"><span class="pre">context</span></code>. (commonly <code class="code docutils literal notranslate"><span class="pre">max_left_words</span></code> or <code class="code docutils literal notranslate"><span class="pre">max_right_words</span></code>)</p>
<p>if the value <code class="code docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code> is passed, the entire <code class="code docutils literal notranslate"><span class="pre">context</span></code> is maintained</p>
</dd>
<dt>first:</dt><dd><p>if <code class="code docutils literal notranslate"><span class="pre">True</span></code>, maintain the first <code class="code docutils literal notranslate"><span class="pre">limit</span></code> words. if <code class="code docutils literal notranslate"><span class="pre">False</span></code> maintain the last <code class="code docutils literal notranslate"><span class="pre">limit</span></code> words</p>
</dd>
</dl>
</dd>
<dt>return:</dt><dd><p>a string containing the context</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.extract_sentences_from_dataset">
<span class="sig-name descname"><span class="pre">extract_sentences_from_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.extract_sentences_from_dataset" title="Permalink to this definition"></a></dt>
<dd><p>organizes the each sentence in a dictionary <code class="code docutils literal notranslate"><span class="pre">{mention_span</span> <span class="pre">:</span> <span class="pre">mention,</span> <span class="pre">left_context_token</span> <span class="pre">:</span> <span class="pre">list_of_left_context_tokens,</span> <span class="pre">right_context_token</span> <span class="pre">:</span> <span class="pre">list_of_right_context_tokens}</span></code> picking the attributed of <code class="code docutils literal notranslate"><span class="pre">dataset</span></code></p>
<dl class="simple">
<dt>parameters:</dt><dd><dl class="simple">
<dt>dataset:</dt><dd><p>see the class parameters <code class="code docutils literal notranslate"><span class="pre">dataset</span></code></p>
</dd>
</dl>
</dd>
<dt>return:</dt><dd><p>a list of dictionaries, one dictionary for each sentence in the dataset</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.make_light">
<span class="sig-name descname"><span class="pre">make_light</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.make_light" title="Permalink to this definition"></a></dt>
<dd><p>delete <code class="code docutils literal notranslate"><span class="pre">dataset</span></code> and <code class="code docutils literal notranslate"><span class="pre">tokenizer</span></code> from the object, this method is useful to save the BaseBERTTokenizedDataset in a light manner, avoiding the original dataset and the tokenizer</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.split_and_cut_mention">
<span class="sig-name descname"><span class="pre">split_and_cut_mention</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mention</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_mention_words</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.split_and_cut_mention" title="Permalink to this definition"></a></dt>
<dd><p>split and cut the mention following the <code class="code docutils literal notranslate"><span class="pre">max_mention_words</span></code> limit</p>
<dl class="simple">
<dt>parameters:</dt><dd><dl class="simple">
<dt>mention:</dt><dd><p>the string to be cutted</p>
</dd>
<dt>max_mention_words:</dt><dd><p>see class parameter <code class="code docutils literal notranslate"><span class="pre">max_mention_words</span></code>. if the value <code class="code docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code> is passed, the entire <code class="code docutils literal notranslate"><span class="pre">mention</span></code> is maintained</p>
</dd>
</dl>
</dd>
<dt>return:</dt><dd><p>a string containing the mention</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.tokenize">
<span class="sig-name descname"><span class="pre">tokenize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sentences</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">80</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.tokenize" title="Permalink to this definition"></a></dt>
<dd><p>tokenizes all sentences using the tokenizer instantiate by <code class="code docutils literal notranslate"><span class="pre">instance_tokenizer</span></code></p>
<dl>
<dt>parameters:</dt><dd><dl>
<dt>sentences:</dt><dd><p>a list of sentences obtained by <code class="code docutils literal notranslate"><span class="pre">create_sentence</span></code> (repeatedly called)</p>
</dd>
<dt>(optional) max_tokens:</dt><dd><p>this param ensures that only the first <code class="code docutils literal notranslate"><span class="pre">max_tokens</span></code> tokens in each tokenized sentence are kept, the other tokens are discarded.</p>
<p>accepted values are integers and the string <code class="code docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code> to avoid the token discard</p>
</dd>
</dl>
</dd>
<dt>return:</dt><dd><p>see <a class="reference external" href="https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/tokenizer#transformers.BatchEncoding">BatchEncoding</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.tokenize_types">
<span class="sig-name descname"><span class="pre">tokenize_types</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.tokenize_types" title="Permalink to this definition"></a></dt>
<dd><p>tokenize alphanumerical types using ids in <code class="code docutils literal notranslate"><span class="pre">type2id</span></code></p>
<dl class="simple">
<dt>parameters:</dt><dd><dl class="simple">
<dt>dataset:</dt><dd><p>see the class parameters <code class="code docutils literal notranslate"><span class="pre">dataset</span></code></p>
</dd>
</dl>
</dd>
<dt>return:</dt><dd><p>a list of list of ids corresponding to the true types of each example in the dataset</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.types2onehot">
<span class="sig-name descname"><span class="pre">types2onehot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_types</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">types</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#entity_typing_framework.dataset_classes.tokenized_datasets.BaseBERTTokenizedDataset.types2onehot" title="Permalink to this definition"></a></dt>
<dd><p>tokenize id of types with one hot encoding</p>
<dl class="simple">
<dt>parameters:</dt><dd><dl class="simple">
<dt>num_types:</dt><dd><p>the number of types in the dataset; automatically extracted by the <a class="reference internal" href="dataset_managers.html"><span class="doc">Dataset Manager</span></a></p>
</dd>
<dt>types:</dt><dd><p>the list of types of each example in the dataset (not the list of all types, but the list of true types for each example in the dataset); automatically extracted by the <a class="reference internal" href="dataset_readers.html"><span class="doc">Dataset Reader</span></a></p>
</dd>
</dl>
</dd>
<dt>return:</dt><dd><p>a torch.tensor with shape <code class="code docutils literal notranslate"><span class="pre">len(types),</span> <span class="pre">num_types</span></code> containing one-hot encodings of types of each example in the dataset</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dataset_readers.html" class="btn btn-neutral float-left" title="Dataset Readers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="datasets.html" class="btn btn-neutral float-right" title="Datasets" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Manuel Vimercati.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>